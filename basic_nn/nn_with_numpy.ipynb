{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z: float) -> float:\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Layer:\n",
    "    weights: np.array\n",
    "    bias: np.array\n",
    "    outputs: np.array\n",
    "    deltas: np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Network:\n",
    "    layers: List[Layer]\n",
    "    learning_rate: Optional[int] = 0.5\n",
    "\n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        return len(self.layers)\n",
    "    \n",
    "    @property\n",
    "    def outputs(self) -> np.array:\n",
    "        return self.layers[-1].outputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(layers: List[int]) -> Network:\n",
    "        layers = [\n",
    "            Layer(\n",
    "                # layers[i] is the number of neurons in layer i (row), layers[i - 1] is the number of weights, matching with number of neurons in layer i - 1 (column)\n",
    "                weights=np.random.rand(layers[i], layers[i - 1]),\n",
    "                bias=np.random.rand(layers[i]),\n",
    "                outputs=np.zeros(layers[i]),\n",
    "                deltas=np.zeros(layers[i]),\n",
    "            )\n",
    "            for i in range(1, len(layers))\n",
    "        ]\n",
    "    \n",
    "        return Network(layers=layers)\n",
    "    \n",
    "def feed_forward(self, inputs: np.array) -> np.array:\n",
    "        for layer in self.layers:\n",
    "            # layer.outputs is a (3,1) - dimension we expect\n",
    "            # layer.weights is a (3,2), inputs is a (2,1) - multiply to get (3,1)\n",
    "            layer.outputs = sigmoid(layer.weights @ inputs + layer.bias)  # == np.matmul, https://stackoverflow.com/a/34142617\n",
    "            inputs = layer.outputs\n",
    "        return self.layers[-1].outputs\n",
    "\n",
    "def back_propagate(self, inputs: np.array, expected: np.array) -> None:\n",
    "    for idx in reversed(range(self.length)):\n",
    "        layer = self.layers[idx]\n",
    "        if idx == len(self.layers) - 1:  # if last layer (output layer)\n",
    "            layer.deltas = (layer.outputs - expected) * sigmoid_derivative(\n",
    "                layer.outputs\n",
    "            )\n",
    "        else:\n",
    "            next_layer = self.layers[idx + 1]\n",
    "            # layer.deltas is a (3,1) - the dimension we expect\n",
    "            # next_layer.weights is a (2,3), next_layer.deltas is a (2,1)\n",
    "            # need to transpose next_layer.weights to get (3,2) then multiply by next_layer.deltas (2,1) to get (3,1)\n",
    "            layer.deltas = (\n",
    "                next_layer.weights.T @ next_layer.deltas\n",
    "                * sigmoid_derivative(layer.outputs)\n",
    "            ) * sigmoid_derivative(layer.outputs)\n",
    "\n",
    "    self.update_weights(inputs)\n",
    "\n",
    "def update_weights(self, inputs: np.array) -> None:\n",
    "    for idx in range(self.length):\n",
    "        layer = self.layers[idx]\n",
    "        previous_layer_outputs = self.layers[idx - 1].outputs if idx > 0 else inputs\n",
    "        # deltas (3,) -> deltas[np.newaxis] (1, 3) -> .T (3, 1)\n",
    "        # previous_layer_outputs (2,) -> previous_layer_outputs[np.newaxis] (1, 2)\n",
    "        # (3,1) @ (1,2) = (3,2) for weights\n",
    "        layer.weights -= (\n",
    "            layer.deltas[np.newaxis].T\n",
    "            @ previous_layer_outputs[np.newaxis]\n",
    "            * self.learning_rate\n",
    "        )\n",
    "        layer.bias -= layer.deltas * self.learning_rate\n",
    "\n",
    "def train(self, inputs: np.array, expected: np.array, epochs: int) -> None:\n",
    "    for epoch in range(epochs):\n",
    "        sum_error = 0.0\n",
    "        for idx, row in enumerate(inputs):\n",
    "            actual = self.feed_forward(row)\n",
    "            self.back_propagate(row, expected[idx])\n",
    "            sum_error += self.mse(actual, expected[idx])\n",
    "        print(f\"Mean squared error: {sum_error}\")\n",
    "        print(f\"epoch={epoch}\")\n",
    "\n",
    "def mse(self, actual: np.array, expected: np.array) -> float:\n",
    "    return np.power(actual - expected, 2).mean()\n",
    "\n",
    "def predict(self, inputs: np.array) -> int:\n",
    "    outputs = self.feed_forward(inputs)\n",
    "    return np.where(outputs == outputs.max())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
