{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic neural network without the help from ML libraries\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(z: float) -> float:\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron Class\n",
    "# weights, biases, delta (backpropagation variable), output\n",
    "\n",
    "@dataclass\n",
    "class Neuron:\n",
    "    weights: List[float]\n",
    "    bias: float\n",
    "    delta: Optional[float] = 0.0\n",
    "    output: Optional[float] = 0.0\n",
    "\n",
    "    def _set_output(self, output: float) -> None:\n",
    "        self.output = output\n",
    "\n",
    "    # probably not needed as after you set the output, you can call it by self.output\n",
    "    def get_output(self) -> float:\n",
    "        return self.output\n",
    "    \n",
    "    def set_delta(self, error: float) -> None:\n",
    "        self.delta = error * sigmoid_derivative(self.output)\n",
    "\n",
    "    def weighted_sum(self, inputs: List[float]) -> float:\n",
    "        # since this is a simple NN, I'll just be multiplying the weights and the inputs\n",
    "        # normally, W should be dot product'ed with the inputs\n",
    "        ws = self.bias\n",
    "        for i in range(len(self.weights)):\n",
    "            ws += self.weights[i] * inputs[i]\n",
    "        return ws\n",
    "\n",
    "    # calculate the output of the neuron by feeding the value to a sigmoid func\n",
    "    def activate(self, inputs: List[float]) -> float:\n",
    "        output = sigmoid(self.weighted_sum(inputs))\n",
    "        self._set_output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building up the layers\n",
    "\n",
    "@dataclass\n",
    "class Layer:\n",
    "    neurons: List[Neuron]\n",
    "\n",
    "    @property\n",
    "    def all_outputs(self) -> List[float]:\n",
    "        return [neuron.output for neuron in self.neurons]\n",
    "\n",
    "    def activate_neurons(self, inputs: List[float]) -> List[float]:\n",
    "        return [neuron.activate(inputs) for neuron in self.neurons]\n",
    "    \n",
    "    def total_delta(self, previous_layer_neuron_idx: int) -> float:\n",
    "        return sum(\n",
    "            neuron.weights[previous_layer_neuron_idx] * neuron.delta\n",
    "            for neuron in self.neurons\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Network:\n",
    "    hidden_layers: List[Layer]\n",
    "    output_layer: Layer\n",
    "    learning_rate:  float\n",
    "\n",
    "    @property\n",
    "    def layers(self) -> List[Layer]:\n",
    "        return self.hidden_layers + [self.output_layer]\n",
    "    \n",
    "    def feed_forward(self, inputs: List[float]) -> List[float]:\n",
    "        for layer in self.hidden_layers:\n",
    "            inputs = layer.activate_neurons(inputs)\n",
    "        return self.output_layer.activate_neurons(inputs)\n",
    "    \n",
    "    def error(self, actual: List[float], expected: List[float]) -> List[float]:\n",
    "        return [actual[i] - expected[i] for i in range(len(actual))]\n",
    "    \n",
    "    def back_propagate(self, inputs: List[float], errors: List[float]) -> None:\n",
    "        # delta is the derivative of the error functions times the derivate of the activation function\n",
    "        for index, neuron in enumerate(self.output_layer.neurons):\n",
    "            neuron.set_delta(errors[index])\n",
    "        \n",
    "        for layer_idx, layer in enumerate(reversed(range(len(self.hidden_layers)))):\n",
    "            next_layer = (\n",
    "                self.output_layer\n",
    "                if layer_idx == len(self.hidden_layers) - 1\n",
    "                else self.hidden_layers[layer_idx + 1]\n",
    "            )\n",
    "            for neuron_idx, neuron in enumerate(layer.neurons):\n",
    "                error_from_next_layer = next_layer.total_delta(neuron_idx)\n",
    "                neuron.set_delta(error_from_next_layer)\n",
    "        \n",
    "        # only update after you've calculated all deltas for all neurons\n",
    "        self.update_weights_for_all_layers(inputs)\n",
    "\n",
    "    def update_weights_for_all_layers(self, inputs: List[float]):\n",
    "        \"\"\"\n",
    "        Update weights for all layers\n",
    "        \"\"\"\n",
    "        # Update weights for hidden layers\n",
    "        for layer_idx in range(len(self.hidden_layers)):\n",
    "            layer = self.hidden_layers[layer_idx]\n",
    "            previous_layer_outputs: List[float] = (\n",
    "                inputs\n",
    "                if layer_idx == 0\n",
    "                else self.hidden_layers[layer_idx - 1].all_outputs\n",
    "            )\n",
    "            for neuron in layer.neurons:\n",
    "                self.update_weights_in_a_layer(previous_layer_outputs, neuron)\n",
    "\n",
    "        # Update weights for output layer\n",
    "        for index, neuron in enumerate(self.output_layer.neurons):\n",
    "            self.update_weights_in_a_layer(self.hidden_layers[-1].all_outputs, neuron)\n",
    "\n",
    "    def update_weights_in_a_layer(\n",
    "        self, previous_layer_outputs: List[float], neuron: Neuron\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Update weights in all neurons in a layer\n",
    "        \"\"\"\n",
    "        for idx in range(len(previous_layer_outputs)):\n",
    "            neuron.weights[idx] -= (\n",
    "                self.learning_rate * neuron.delta * previous_layer_outputs[idx]\n",
    "            )\n",
    "            neuron.bias -= self.learning_rate * neuron.delta\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_epoch: int,\n",
    "        num_outputs: int,\n",
    "        training_set: List[List[float]],\n",
    "        training_output: List[float],\n",
    "    ) -> None:\n",
    "        for epoch in range(num_epoch):\n",
    "            sum_error = 0.0\n",
    "            for idx, row in enumerate(training_set):\n",
    "                expected = [0 for _ in range(num_outputs)]\n",
    "                expected[training_output[idx]] = 1  # one-hot encoding\n",
    "                actual = self.feed_forward(row)\n",
    "                errors = self.derivative_error_to_output(actual, expected)\n",
    "                self.back_propagate(row, errors)\n",
    "                sum_error += self.mse(actual, training_output)\n",
    "            print(f\"Mean squared error: {sum_error}\")\n",
    "            print(f\"epoch={epoch}\")\n",
    "\n",
    "    def predict(self, inputs: List[float]) -> int:\n",
    "        outputs = self.feed_forward(inputs)\n",
    "        return outputs.index(max(outputs))\n",
    "\n",
    "    def mse(self, actual: List[float], expected: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Mean Squared Error formula\n",
    "        \"\"\"\n",
    "        return sum((actual[i] - expected[i]) ** 2 for i in range(len(actual))) / len(\n",
    "            actual\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'random.random(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Got=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (expected[i], prediction))\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mtest_make_prediction_with_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mtest_make_prediction_with_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m n_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     20\u001b[0m n_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(expected))\n\u001b[1;32m     21\u001b[0m hidden_layers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m     Layer(\n\u001b[1;32m     23\u001b[0m         neurons\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 24\u001b[0m             Neuron(weights\u001b[38;5;241m=\u001b[39m[\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_inputs)], bias\u001b[38;5;241m=\u001b[39mrandom()),\n\u001b[1;32m     25\u001b[0m             Neuron(weights\u001b[38;5;241m=\u001b[39m[random() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_inputs)], bias\u001b[38;5;241m=\u001b[39mrandom()),\n\u001b[1;32m     26\u001b[0m         ],\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     29\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m Layer(\n\u001b[1;32m     30\u001b[0m     neurons\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     31\u001b[0m         Neuron(weights\u001b[38;5;241m=\u001b[39m[random() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_outputs)], bias\u001b[38;5;241m=\u001b[39mrandom()),\n\u001b[1;32m     32\u001b[0m         Neuron(weights\u001b[38;5;241m=\u001b[39m[random() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_outputs)], bias\u001b[38;5;241m=\u001b[39mrandom()),\n\u001b[1;32m     33\u001b[0m     ],\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m network \u001b[38;5;241m=\u001b[39m Network(\n\u001b[1;32m     36\u001b[0m     hidden_layers\u001b[38;5;241m=\u001b[39mhidden_layers, output_layer\u001b[38;5;241m=\u001b[39moutput_layer, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'random.random(...)'?"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def test_make_prediction_with_network():\n",
    "    # Test making predictions with the network\n",
    "    # Mock data is from https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "    dataset = [\n",
    "        [2.7810836, 2.550537003],\n",
    "        [1.465489372, 2.362125076],\n",
    "        [3.396561688, 4.400293529],\n",
    "        [1.38807019, 1.850220317],\n",
    "        [3.06407232, 3.005305973],\n",
    "        [7.627531214, 2.759262235],\n",
    "        [5.332441248, 2.088626775],\n",
    "        [6.922596716, 1.77106367],\n",
    "        [8.675418651, -0.242068655],\n",
    "        [7.673756466, 3.508563011],\n",
    "    ]\n",
    "    expected = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "    n_inputs = len(dataset[0])\n",
    "    n_outputs = len(set(expected))\n",
    "    hidden_layers = [\n",
    "        Layer(\n",
    "            neurons=[\n",
    "                Neuron(weights=[random() for _ in range(n_inputs)], bias=random()),\n",
    "                Neuron(weights=[random() for _ in range(n_inputs)], bias=random()),\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "    output_layer = Layer(\n",
    "        neurons=[\n",
    "            Neuron(weights=[random() for _ in range(n_outputs)], bias=random()),\n",
    "            Neuron(weights=[random() for _ in range(n_outputs)], bias=random()),\n",
    "        ],\n",
    "    )\n",
    "    network = Network(\n",
    "        hidden_layers=hidden_layers, output_layer=output_layer, learning_rate=0.5\n",
    "    )\n",
    "    network.train(40, n_outputs, dataset, expected)\n",
    "    print(f\"Hidden layer: {network.layers[0].neurons}\")\n",
    "    print(f\"Output layer: {network.layers[1].neurons}\")\n",
    "    \n",
    "    # This is just for demonstration only\n",
    "    for i in range(len(dataset)):\n",
    "        prediction = network.predict(dataset[i])\n",
    "        print(\"Expected=%d, Got=%d\" % (expected[i], prediction))\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test_make_prediction_with_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
